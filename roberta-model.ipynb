{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":7321404,"sourceType":"datasetVersion","datasetId":4248812},{"sourceId":7402911,"sourceType":"datasetVersion","datasetId":4304853},{"sourceId":7402933,"sourceType":"datasetVersion","datasetId":4304868},{"sourceId":7408043,"sourceType":"datasetVersion","datasetId":4308529}],"dockerImageVersionId":30636,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport os\nfrom torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler, SequentialSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom tqdm import tqdm\n\n# Paths for datasets and models\ndataset_path = '/kaggle/input/mit-dataset/final_train.csv'\nmodel_save_path = '/kaggle/input/saved-model-roberta'\ntokenized_data_path = '/kaggle/input/token-vals/tokenized_data.pt'\n\n# Check and create model directory if not exists\nif not os.path.exists(model_save_path):\n    os.makedirs(model_save_path)\n\n# Load tokenizer\ntokenizer_path = '/kaggle/input/tokenizer-2'\nif os.path.exists(tokenizer_path):\n    tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path)\nelse:\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n    tokenizer.save_pretrained(tokenizer_path)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to tokenize and encode the dataset using RoBERTa tokenizer\ndef roberta_encode(texts, tokenizer, max_len=512):\n    input_ids = []\n    attention_masks = []\n\n    for text in tqdm(texts, desc=\"Tokenizing\"):\n        encoded = tokenizer.encode_plus(\n            text, \n            max_length=max_len, \n            add_special_tokens=True,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        \n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    \n    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n\n# Load and preprocess data\nif os.path.exists(tokenized_data_path):\n    tokenized_data = torch.load(tokenized_data_path)\n    X_train_encoded, X_train_mask, y_train_tensor = tokenized_data['train_input_ids'], tokenized_data['train_attention_masks'], tokenized_data['train_labels']\n    X_val_encoded, X_val_mask, y_val_tensor = tokenized_data['val_input_ids'], tokenized_data['val_attention_masks'], tokenized_data['val_labels']\nelse:\n    df = pd.read_csv(dataset_path)\n    X = df['text']\n    y = df['label']\n\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n    X_train_encoded, X_train_mask = roberta_encode(X_train, tokenizer)\n    X_val_encoded, X_val_mask = roberta_encode(X_val, tokenizer)\n\n    y_train_tensor = torch.tensor(y_train.values)\n    y_val_tensor = torch.tensor(y_val.values)\n\n    # Save the tokenized data\n    torch.save({\n        'train_input_ids': X_train_encoded,\n        'train_attention_masks': X_train_mask,\n        'train_labels': y_train_tensor,\n        'val_input_ids': X_val_encoded,\n        'val_attention_masks': X_val_mask,\n        'val_labels': y_val_tensor\n    }, tokenized_data_path)\n\n# Creating TensorDatasets and DataLoaders\ntrain_dataset = TensorDataset(X_train_encoded, X_train_mask, y_train_tensor)\nval_dataset = TensorDataset(X_val_encoded, X_val_mask, y_val_tensor)\n\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train_tensor.numpy()), y=y_train_tensor.numpy())\nweights = torch.tensor(class_weights, dtype=torch.float)\ntrain_sampler = WeightedRandomSampler(weights=weights[y_train_tensor.long()], num_samples=len(y_train_tensor), replacement=True)\n\ntrain_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=16)\nval_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\nimport torch.cuda.amp as amp  # For mixed-precision training\n\n# Function to save the model\ndef save_model(model, model_dir):\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    model.save_pretrained(model_dir)\n\n# Function to initialize or load the model\ndef initialize_model(model_dir):\n    if os.path.exists(model_dir) and \"config.json\" in os.listdir(model_dir):\n        model = RobertaForSequenceClassification.from_pretrained(model_dir, num_labels=2)\n    else:\n        model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n        save_model(model, model_dir)\n    return model\n\nmodel = initialize_model(model_save_path)\n\n# Mixed-precision training setup\nscaler = amp.GradScaler()\n\n# Initialize optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 4) # Adjust epochs as necessary\n\n# Function to train the model with mixed-precision\ndef train(model, train_dataloader, val_dataloader, optimizer, scheduler, scaler, device, model_dir, epochs=4):\n    best_val_accuracy = float('-inf')\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} - Training\")\n\n        for step, batch in enumerate(train_progress_bar):\n            batch_input_ids = batch[0].to(device)\n            batch_input_mask = batch[1].to(device)\n            batch_labels = batch[2].to(device)\n\n            model.zero_grad()\n\n            with amp.autocast():\n                outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n                loss = outputs.loss\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n\n            total_loss += loss.item()\n            train_progress_bar.set_postfix({'Loss': loss.item()})\n\n        avg_train_loss = total_loss / len(train_dataloader)\n        avg_val_accuracy, avg_val_loss = evaluate(model, val_dataloader, device)\n\n        print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy:.4f}\")\n    print(f\"Best Validation Accuracy: {best_val_accuracy}\")\n\n\n        # Check and save the best model\n    if avg_val_accuracy > best_val_accuracy:\n            best_val_accuracy = avg_val_accuracy\n            save_model(model, model_dir)\n\n# Function for evaluation\ndef evaluate(model, dataloader, device):\n    model.eval()\n    val_accuracy = []\n    val_loss = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            batch_input_ids = batch[0].to(device)\n            batch_input_mask = batch[1].to(device)\n            batch_labels = batch[2].to(device)\n\n            with amp.autocast():\n                outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n                loss = outputs.loss\n                logits = outputs.logits\n\n            val_loss += loss.item()\n            predictions = torch.argmax(logits, dim=-1)\n            accuracy = accuracy_score(batch_labels.cpu().numpy(), predictions.cpu().numpy())\n            val_accuracy.append(accuracy)\n\n    avg_val_loss = val_loss / len(dataloader)\n    avg_val_accuracy = sum(val_accuracy) / len(val_accuracy)\n    return avg_val_accuracy, avg_val_loss\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to tokenize and encode the dataset using RoBERTa tokenizer\ndef roberta_encode(texts, tokenizer, max_len=512):\n    input_ids = []\n    attention_masks = []\n\n    for text in tqdm(texts, desc=\"Tokenizing\"):\n        encoded = tokenizer.encode_plus(\n            text, \n            max_length=max_len, \n            add_special_tokens=True,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        \n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    \n    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n\n# Load and preprocess data\nif os.path.exists(tokenized_data_path):\n    tokenized_data = torch.load(tokenized_data_path)\n    X_train_encoded, X_train_mask, y_train_tensor = tokenized_data['train_input_ids'], tokenized_data['train_attention_masks'], tokenized_data['train_labels']\n    X_val_encoded, X_val_mask, y_val_tensor = tokenized_data['val_input_ids'], tokenized_data['val_attention_masks'], tokenized_data['val_labels']\nelse:\n    df = pd.read_csv(dataset_path)\n    X = df['text']\n    y = df['label']\n\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n    X_train_encoded, X_train_mask = roberta_encode(X_train, tokenizer)\n    X_val_encoded, X_val_mask = roberta_encode(X_val, tokenizer)\n\n    y_train_tensor = torch.tensor(y_train.values)\n    y_val_tensor = torch.tensor(y_val.values)\n\n    # Save the tokenized data\n    torch.save({\n        'train_input_ids': X_train_encoded,\n        'train_attention_masks': X_train_mask,\n        'train_labels': y_train_tensor,\n        'val_input_ids': X_val_encoded,\n        'val_attention_masks': X_val_mask,\n        'val_labels': y_val_tensor\n    }, tokenized_data_path)\n\n# Creating TensorDatasets and DataLoaders\ntrain_dataset = TensorDataset(X_train_encoded, X_train_mask, y_train_tensor)\nval_dataset = TensorDataset(X_val_encoded, X_val_mask, y_val_tensor)\n\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train_tensor.numpy()), y=y_train_tensor.numpy())\nweights = torch.tensor(class_weights, dtype=torch.float)\ntrain_sampler = WeightedRandomSampler(weights=weights[y_train_tensor.long()], num_samples=len(y_train_tensor), replacement=True)\n\ntrain_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=16)\nval_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\nimport torch.cuda.amp as amp  # For mixed-precision training\n\n# Function to initialize or load the model\ndef initialize_model(model_path):\n    if os.path.exists(model_path):\n        model = RobertaForSequenceClassification.from_pretrained(model_path, num_labels=2) # Adjust num_labels based on your task\n    else:\n        model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2) # Adjust num_labels based on your task\n    return model\n\nmodel = initialize_model(model_save_path)\n\n# Mixed-precision training setup\nscaler = amp.GradScaler()\n\n# Initialize optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 4) # Adjust epochs if necessary\n\n# Function to train the model with mixed-precision\ndef train(model, train_dataloader, val_dataloader, optimizer, scheduler, scaler, device, model_save_path, epochs):\n    best_val_accuracy = float('-inf')\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} - Training\")\n\n        for step, batch in enumerate(train_progress_bar):\n            batch_input_ids = batch[0].to(device)\n            batch_input_mask = batch[1].to(device)\n            batch_labels = batch[2].to(device)\n\n            model.zero_grad()\n\n            with amp.autocast():\n                outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n                loss = outputs.loss\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n\n            total_loss += loss.item()\n            train_progress_bar.set_postfix({'Loss': loss.item()})\n\n        avg_train_loss = total_loss / len(train_dataloader)\n        avg_val_accuracy, avg_val_loss = evaluate(model, val_dataloader, device)\n\n        print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy:.4f}\")\n\n        if avg_val_accuracy > best_val_accuracy:\n            best_val_accuracy = avg_val_accuracy\n            model.save_pretrained(model_save_path)\n\n    print(f\"Best Validation Accuracy: {best_val_accuracy}\")\n\n# Function for evaluation\ndef evaluate(model, dataloader, device):\n    model.eval()\n    val_accuracy = []\n    val_loss = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            batch_input_ids = batch[0].to(device)\n            batch_input_mask = batch[1].to(device)\n            batch_labels = batch[2].to(device)\n\n            with amp.autocast():\n                outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n                loss = outputs.loss\n                logits = outputs.logits\n\n            val_loss += loss.item()\n            predictions = torch.argmax(logits, dim=-1)\n            accuracy = accuracy_score(batch_labels.cpu().numpy(), predictions.cpu().numpy())\n            val_accuracy.append(accuracy)\n\n    avg_val_loss = val_loss / len(dataloader)\n    avg_val_accuracy = sum(val_accuracy) / len(val_accuracy)\n    return avg_val_accuracy, avg_val_loss\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to encode test data\ndef encode_test_data(test_texts, tokenizer):\n    input_ids = []\n    attention_masks = []\n\n    for text in tqdm(test_texts, desc=\"Encoding Test Data\"):\n        encoded = tokenizer.encode_plus(\n            text, \n            max_length=512,  # Adjust the max length as needed\n            add_special_tokens=True,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        \n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    \n    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n\n# Function to create submission file\ndef create_submission_file(model, dataloader, submission_file_path, ids):\n    model.eval()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    predictions = []\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Generating Predictions\"):\n            batch_input_ids = batch[0].to(device)\n            batch_input_mask = batch[1].to(device)\n\n            with amp.autocast():\n                outputs = model(batch_input_ids, attention_mask=batch_input_mask)\n            logits = outputs.logits\n            probs = torch.nn.functional.softmax(logits, dim=-1)\n            predictions.extend(probs[:,1].detach().cpu().numpy())  # Adjust as per the specific task\n\n    submission_df = pd.DataFrame({'id': ids, 'generated': predictions})\n    submission_df.to_csv(submission_file_path, index=False)\n\n# Loading test data and prepare for prediction\ntest_data_path = '/kaggle/input/llm-detect-ai-generated-text/test_essays.csv'  # Adjust path as needed\ntest_data = pd.read_csv(test_data_path)\nX_test_encoded, X_test_mask = encode_test_data(test_data['text'], tokenizer)  # Adjust column name as needed\ntest_dataset = TensorDataset(X_test_encoded, X_test_mask)\ntest_dataloader = DataLoader(test_dataset, batch_size=16)\n\n# Path for saving the submission file\nsubmission_file_path = '/kaggle/working/submission.csv'\n\n# Generating and save predictions in submission file\ncreate_submission_file(model, test_dataloader, submission_file_path, test_data['id'])  # Adjust column name as needed\n","metadata":{},"execution_count":null,"outputs":[]}]}