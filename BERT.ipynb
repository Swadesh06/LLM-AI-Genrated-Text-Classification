{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":7385896,"sourceType":"datasetVersion","datasetId":4292994},{"sourceId":7401512,"sourceType":"datasetVersion","datasetId":4303871}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-15T15:18:15.504171Z","iopub.execute_input":"2024-01-15T15:18:15.504549Z","iopub.status.idle":"2024-01-15T15:18:15.513562Z","shell.execute_reply.started":"2024-01-15T15:18:15.504519Z","shell.execute_reply":"2024-01-15T15:18:15.512562Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/input/tokenized-d/tokenized_data.pt\n/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv\n/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\n/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\n/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\n/kaggle/input/maindata1/final_train.csv\n/kaggle/input/maindata1/final_test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport os\nfrom torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler, SequentialSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\n\n\ndef optimal_max_len(texts, tokenizer, percentile=0.95):\n    token_lengths = [len(tokenizer.encode(text, add_special_tokens=True)) for text in texts]\n    return int(np.percentile(token_lengths, percentile))\n\noptimal_max_len = find_optimal_max_len(X_train, tokenizer)\nprint(f\"Optimal maximum length: {optimal_max_len}\")\n\n\n# Tokenization function using BERT tokenizer\ndef bert_encode(texts, tokenizer, max_len=None):\n    if max_len is None:\n        max_len = optimal_max_len(texts, tokenizer)\n    input_ids = []\n    attention_masks = []\n\n    for text in tqdm(texts, desc=\"Tokenizing\"):\n        encoded = tokenizer.encode_plus(\n            text, \n            max_length=max_len, \n            add_special_tokens=True,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        \n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    \n    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n\n# Improved training function with BERT model\ndef train(model, train_dataloader, val_dataloader, optimizer, scheduler, model_save_dir, epochs=4):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    best_val_accuracy = float('-inf')\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} - Training\")\n\n        for step, batch in enumerate(train_progress_bar):\n            batch_input_ids = batch[0].to(device)\n            batch_input_mask = batch[1].to(device)\n            batch_labels = batch[2].to(device)\n\n            model.zero_grad()\n            outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n\n            train_progress_bar.set_postfix({'Loss': loss.item()})\n\n        avg_train_loss = total_loss / len(train_dataloader)\n        model.eval()\n        val_accuracy = []\n        val_loss = 0\n        val_progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\")\n\n        with torch.no_grad():\n            for batch in val_progress_bar:\n                batch_input_ids = batch[0].to(device)\n                batch_input_mask = batch[1].to(device)\n                batch_labels = batch[2].to(device)\n\n                outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n                loss = outputs.loss\n                val_loss += loss.item()\n\n                logits = outputs.logits\n                predictions = torch.argmax(logits, dim=-1)\n                accuracy = accuracy_score(batch_labels.cpu().numpy(), predictions.cpu().numpy())\n                val_accuracy.append(accuracy)\n\n                val_progress_bar.set_postfix({'Loss': loss.item()})\n\n        avg_val_loss = val_loss / len(val_dataloader)\n        avg_val_accuracy = sum(val_accuracy) / len(val_accuracy)\n        print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy:.4f}\")\n\n        if avg_val_accuracy > best_val_accuracy:\n            best\n            best_val_accuracy = avg_val_accuracy\n            # Check if save directory exists, if not, create it\n            if not os.path.exists(model_save_dir):\n                os.makedirs(model_save_dir)\n            # Save the model\n            torch.save(model.state_dict(), os.path.join(model_save_dir, 'best_model.pth'))\n\n    print(f\"Best Validation Accuracy: {best_val_accuracy}\")\n\n\n\n# Function to load model weights\ndef load_best_model(model_path):\n    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\n    return model\n\n# Paths for datasets and models\ndataset_path = '/kaggle/input/maindata1/final_train.csv'\ntokenized_data_path = '/kaggle/input/tokenized-d/tokenized_data.pt'\nmodel_save_path = '/kaggle/working'\n\n# Load tokenizer and model\ntokenizer_path = '/kaggle/working/tokenizer'\nif os.path.exists(tokenizer_path):\n    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\nelse:\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    tokenizer.save_pretrained(tokenizer_path)\n\nmodel_path = f\"{model_save_path}/best_model.pth\"\n\nif os.path.exists(model_path):\n    model = BertForSequenceClassification.from_pretrained(model_path)\nelse:\n    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n\n# Load and preprocess data\nif os.path.exists(tokenized_data_path):\n    data = torch.load(tokenized_data_path)\n    X_train_encoded, X_train_mask, y_train_tensor = data['train_input_ids'], data['train_attention_masks'], data['train_labels']\n    X_val_encoded, X_val_mask, y_val_tensor = data['val_input_ids'], data['val_attention_masks'], data['val_labels']\nelse:\n    train_essays = pd.read_csv(dataset_path)\n    X = train_essays['text']\n    y = train_essays['label']\n\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n    X_train_encoded, X_train_mask = bert_encode(X_train, tokenizer)\n    X_val_encoded, X_val_mask = bert_encode(X_val, tokenizer)\n\n    y_train_tensor = torch.tensor(y_train.values)\n    y_val_tensor = torch.tensor(y_val.values)\n\n    torch.save({\n        'train_input_ids': X_train_encoded,\n        'train_attention_masks': X_train_mask,\n        'train_labels': y_train_tensor,\n        'val_input_ids': X_val_encoded,\n        'val_attention_masks': X_val_mask,\n        'val_labels': y_val_tensor\n    }, tokenized_data_path)\n\n# Handling class imbalance\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train_tensor), y=y_train_tensor.numpy())\nweights = torch.tensor(class_weights, dtype=torch.float)\ntrain_sampler = WeightedRandomSampler(weights=weights[y_train_tensor.long()], num_samples=len(y_train_tensor), replacement=True)\n\ntrain_dataset = TensorDataset(X_train_encoded, X_train_mask, y_train_tensor)\nval_dataset = TensorDataset(X_val_encoded, X_val_mask, y_val_tensor)\n\ntrain_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=16)\nval_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)\n\n# Initialize optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 4)\n\n# Function to save tokenized data\ndef save_tokenized_data(input_ids, attention_masks, labels, file_path):\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    torch.save({\n        'input_ids': input_ids,\n        'attention_masks': attention_masks,\n        'labels': labels\n    }, file_path)\n\n# Function to load tokenized data\ndef load_tokenized_data(file_path):\n    data = torch.load(file_path)\n    return data['input_ids'], data['attention_masks'], data['labels']\n\n# Check if saved model and tokenized data exist, and load them\ntokenized_train_path = '/kaggle/working/tokenized_train_data.pt'\ntokenized_val_path = '/kaggle/working/tokenized_val_data.pt'\n\nmodel_path = '/kaggle/working/best_model.pth'\n\n# Check if the model file exists\nif os.path.exists(model_path):\n    model.load_state_dict(torch.load(model_path))\n    model.eval()\nelse:\n    print(\"Model file not found. Training the model.\")\n    # Train the model\n    train(model, train_dataloader, val_dataloader, optimizer, scheduler, '/kaggle/working', epochs=1)\n    # Save the trained model\n    torch.save(model.state_dict(), model_path)\n# Load or preprocess and save tokenized data\nif os.path.exists(tokenized_train_path) and os.path.exists(tokenized_val_path):\n    X_train_encoded, X_train_mask, y_train_tensor = load_tokenized_data(tokenized_train_path)\n    X_val_encoded, X_val_mask, y_val_tensor = load_tokenized_data(tokenized_val_path)\nelse:\n    # Tokenize and save training data\n    X_train_encoded, X_train_mask = bert_encode(X_train, tokenizer)\n    y_train_tensor = torch.tensor(y_train.values)\n    save_tokenized_data(X_train_encoded, X_train_mask, y_train_tensor, tokenized_train_path)\n\n    # Tokenize and save validation data\n    X_val_encoded, X_val_mask = bert_encode(X_val, tokenizer)\n    y_val_tensor = torch.tensor(y_val.values)\n    save_tokenized_data(X_val_encoded, X_val_mask, y_val_tensor, tokenized_val_path)\n\n# Load the best model for inference\nload_best_model(model, f\"{model_path}/best_model.pth\")\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train_tensor), y=y_train_tensor.numpy())\nweights = torch.tensor(class_weights, dtype=torch.float)\ntrain_sampler = WeightedRandomSampler(weights=weights[y_train_tensor.long()], num_samples=len(y_train_tensor), replacement=True)\n\ntrain_dataset = TensorDataset(X_train_encoded, X_train_mask, y_train_tensor)\nval_dataset = TensorDataset(X_val_encoded, X_val_mask, y_val_tensor)\n\ntrain_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=16)\nval_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)\n\n\n\ntest_data_path = '/kaggle/input/llm-detect-ai-generated-text/test_essays.csv'\ntest_data = pd.read_csv(test_data_path)\nX_test_encoded, X_test_mask = bert_encode(test_data['text'], tokenizer)\ndummy_labels = torch.zeros(len(X_test_encoded))\n\ntest_dataset = TensorDataset(X_test_encoded, X_test_mask, dummy_labels)\ntest_dataloader = DataLoader(test_dataset, batch_size=16)\n\n\ndef create_submission_file(model, dataloader, submission_file_path, test_data):\n    model.eval()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    predictions = []\n\n    for batch in tqdm(dataloader, desc=\"Generating Predictions\"):\n        batch_input_ids = batch[0].to(device)\n        batch_input_mask = batch[1].to(device)\n\n        with torch.no_grad():\n            outputs = model(batch_input_ids, attention_mask=batch_input_mask)\n        logits = outputs.logits\n        probs = torch.nn.functional.softmax(logits, dim=-1)\n        predictions.extend(probs[:,1].detach().cpu().numpy())\n\n    submission_df = pd.DataFrame({'id': test_data['id'], 'generated': predictions})\n    submission_df.to_csv(submission_file_path, index=False)\n\n# Call the function to create the submission file\ncreate_submission_file(model, test_dataloader, '/kaggle/working/submission.csv', test_data)","metadata":{"execution":{"iopub.status.busy":"2024-01-15T15:24:53.631036Z","iopub.execute_input":"2024-01-15T15:24:53.631495Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model file not found. Training the model.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1 - Training:  75%|███████▍  | 12976/17349 [3:02:47<1:01:41,  1.18it/s, Loss=1.13e-5] ","output_type":"stream"}]},{"cell_type":"markdown","source":"## BERT with optimization for overfitting and reduced complexity of task","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Paths for datasets and tokenizer\ndataset_path = '/kaggle/input/maindata1/final_train.csv'\ntokenizer_path = '/kaggle/working/tokenizer'\n\n# Loading tokenizer\nif os.path.exists(tokenizer_path):\n    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\nelse:\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    tokenizer.save_pretrained(tokenizer_path)\n\n# Loading and preprocessing data\ntrain_essays = pd.read_csv(dataset_path)\nX = train_essays['text']\ny = train_essays['label']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenization function using BERT tokenizer\ndef bert_encode(texts, tokenizer, max_len=128):  # Reduced max_len for smaller datasets\n    input_ids = []\n    attention_masks = []\n\n    for text in texts:\n        encoded = tokenizer.encode_plus(\n            text,\n            max_length=max_len,\n            add_special_tokens=True,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        \n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    \n    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n\n# Tokenizing the data\nX_train_encoded, X_train_mask = bert_encode(X_train, tokenizer)\nX_val_encoded, X_val_mask = bert_encode(X_val, tokenizer)\n\n# Converting labels to tensors\ny_train_tensor = torch.tensor(y_train.values)\ny_val_tensor = torch.tensor(y_val.values)\n\n# Create TensorDatasets\ntrain_dataset = TensorDataset(X_train_encoded, X_train_mask, y_train_tensor)\nval_dataset = TensorDataset(X_val_encoded, X_val_mask, y_val_tensor)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, SequentialSampler, WeightedRandomSampler\nfrom transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nimport os\n\n# Handling class imbalance\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train_tensor), y=y_train_tensor.numpy())\nweights = torch.tensor(class_weights, dtype=torch.float)\ntrain_sampler = WeightedRandomSampler(weights=weights[y_train_tensor.long()], num_samples=len(y_train_tensor), replacement=True)\n\n# Creating DataLoaders\ntrain_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=16)\nval_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)\n\n# Initializing BERT model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(np.unique(y_train_tensor)))\n\n# Model save path\nmodel_save_path = '/kaggle/working/best_model.pth'\n\n# Checking if the model is already trained and saved\nif os.path.exists(model_save_path):\n    model.load_state_dict(torch.load(model_save_path))\nelse:\n    # Initializing optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)  # Small learning rate for fine-tuning\n    total_steps = len(train_dataloader) * 4  \n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nfrom tqdm import tqdm\nimport copy\n\ndef train(model, train_dataloader, val_dataloader, optimizer, scheduler, model_save_dir, epochs=4, patience=2):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    best_val_loss = float('inf')\n    no_improve_epochs = 0  # Counter for early stopping\n    best_model = copy.deepcopy(model.state_dict())  # To save the best model\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n\n        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n            batch_input_ids = batch[0].to(device)\n            batch_input_mask = batch[1].to(device)\n            batch_labels = batch[2].to(device)\n\n            model.zero_grad()\n            outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n            optimizer.step()\n            scheduler.step()\n\n        avg_train_loss = total_loss / len(train_dataloader)\n\n        # Validation phase\n        model.eval()\n        val_loss = 0\n\n        with torch.no_grad():\n            for batch in tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n                batch_input_ids = batch[0].to(device)\n                batch_input_mask = batch[1].to(device)\n                batch_labels = batch[2].to(device)\n\n                outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n                loss = outputs.loss\n                val_loss += loss.item()\n\n        avg_val_loss = val_loss / len(val_dataloader)\n        print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n\n        # Checking if validation loss improved\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            no_improve_epochs = 0\n            best_model = copy.deepcopy(model.state_dict())\n            # Save the best model\n            if not os.path.exists(model_save_dir):\n                os.makedirs(model_save_dir)\n            torch.save(model.state_dict(), os.path.join(model_save_dir, 'best_model.pth'))\n        else:\n            no_improve_epochs += 1\n            if no_improve_epochs >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    # Load the best model before returning\n    model.load_state_dict(best_model)\n    return model\n\nif not os.path.exists(model_save_path):\n    trained_model = train(model, train_dataloader, val_dataloader, optimizer, scheduler, '/kaggle/working', epochs=4, patience=2)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_submission_file(model, dataloader, submission_file_path):\n    model.eval()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    predictions = []\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Generating Predictions\"):\n            batch_input_ids = batch[0].to(device)\n            batch_input_mask = batch[1].to(device)\n\n            outputs = model(batch_input_ids, attention_mask=batch_input_mask)\n            logits = outputs.logits\n            probs = torch.nn.functional.softmax(logits, dim=-1)\n            predictions.extend(probs[:,1].detach().cpu().numpy())\n\n    submission_df = pd.DataFrame({'id': test_data['id'], 'generated': predictions})\n    submission_df.to_csv(submission_file_path, index=False)\n\n# Load the test data\ntest_data_path = '/kaggle/input/llm-detect-ai-generated-text/test_essays.csv'\ntest_data = pd.read_csv(test_data_path)\n\n# Tokenize test data\nX_test_encoded, X_test_mask = bert_encode(test_data['text'], tokenizer)\ndummy_labels = torch.zeros(len(X_test_encoded))  # Dummy labels for dataloader\n\n# Create test dataset and dataloader\ntest_dataset = TensorDataset(X_test_encoded, X_test_mask, dummy_labels)\ntest_dataloader = DataLoader(test_dataset, batch_size=16)\n\n# Generate submission file\nsubmission_file_path = '/kaggle/working/submission.csv'\ncreate_submission_file(trained_model, test_dataloader, submission_file_path)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}